#!/usr/bin/env python3
"""
GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
MenZ-ReazonSpeechã§åˆ©ç”¨å¯èƒ½ãªGPUç’°å¢ƒã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import sys
import platform
import torch
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from reazon_speech.config import ModelConfig
from reazon_speech.utils import get_device_info


def check_python_environment():
    """Pythonç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ Pythonç’°å¢ƒ:")
    print(f" ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    print(f" ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {platform.platform()}")
    print()


def check_pytorch():
    """PyTorchç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ”¥ PyTorchç’°å¢ƒ:")
    try:
        print(f" PyTorch: {torch.__version__}")
        print(f" CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f" CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
            gpu_count = torch.cuda.device_count()
            print(f" GPUãƒ‡ãƒã‚¤ã‚¹æ•°: {gpu_count}")
            print()
            
            print(" åˆ©ç”¨å¯èƒ½ãªGPU:")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_properties = torch.cuda.get_device_properties(i)
                gpu_memory = gpu_properties.total_memory / (1024**3)
                compute_capability = f"{gpu_properties.major}.{gpu_properties.minor}"
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
                try:
                    torch.cuda.empty_cache()
                    allocated = torch.cuda.memory_allocated(i) / (1024**3)
                    cached = torch.cuda.memory_reserved(i) / (1024**3)
                    free = gpu_memory - cached
                    print(f" GPU {i}: {gpu_name}")
                    print(f" ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB (ä½¿ç”¨ä¸­: {cached:.1f}GB, ç©ºã: {free:.1f}GB)")
                    print(f" Compute Capability: {compute_capability}")
                    print(f" è¨­å®šä¾‹: device = cuda, gpu_id = {i}")
                except Exception:
                    print(f" GPU {i}: {gpu_name}")
                    print(f" ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB")
                    print(f" Compute Capability: {compute_capability}")
                    print(f" è¨­å®šä¾‹: device = cuda, gpu_id = {i}")
                print()
            
            if gpu_count > 1:
                print(" ğŸ” è¤‡æ•°GPUãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼")
                print(" è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§GPU IDã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ç‰¹å®šã®GPUã‚’ä½¿ç”¨ã§ãã¾ã™:")
                print(" config.ini ã® [inference] ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§")
                print(" device = cuda")
                print(" gpu_id = 0 # ä½¿ç”¨ã—ãŸã„GPUã®IDï¼ˆ0ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰")
                print()
        
        # Apple Silicon (MPS) ãƒã‚§ãƒƒã‚¯
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print(" MPSï¼ˆApple Siliconï¼‰: åˆ©ç”¨å¯èƒ½")
            print()
            
    except ImportError:
        print(" âŒ PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(" pip install torch ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print()


def check_nemo():
    """NeMoç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ¯ NeMoç’°å¢ƒ:")
    try:
        import nemo
        print(f" NeMo: {nemo.__version__}")
        print()
    except ImportError:
        print(" âŒ NeMoãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(" pip install nemo_toolkit[asr] ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print()


def check_audio_libraries():
    """éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒã‚§ãƒƒã‚¯"""
    print("ğŸµ éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
    
    # sounddevice
    try:
        import sounddevice as sd
        print(f" sounddevice: {sd.__version__}")
    except ImportError:
        print(" âŒ sounddeviceãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(" pip install sounddevice ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    # librosa
    try:
        import librosa
        print(f" librosa: {librosa.__version__}")
    except ImportError:
        print(" âŒ librosaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(" pip install librosa ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    # numpy
    try:
        import numpy as np
        print(f" numpy: {np.__version__}")
    except ImportError:
        print(" âŒ numpyãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(" pip install numpy ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    
    print()


def check_system_resources():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹:")
    try:
        import psutil
        
        # CPUæƒ…å ±
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f" CPU: {cpu_count}ã‚³ã‚¢ (ä½¿ç”¨ç‡: {cpu_percent}%)")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        memory_percent = memory.percent
        print(f" ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f}GB (ä½¿ç”¨ç‡: {memory_percent}%)")
        
        if memory_gb < 4:
            print(" âš ï¸ ãƒ¡ãƒ¢ãƒªãŒ4GBæœªæº€ã§ã™ã€‚å‹•ä½œãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        elif memory_gb < 8:
            print(" âš ï¸ ãƒ¡ãƒ¢ãƒªãŒ8GBæœªæº€ã§ã™ã€‚å¤§ããªãƒ¢ãƒ‡ãƒ«ã§å•é¡ŒãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        
        # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
        disk = psutil.disk_usage('.')
        disk_gb = disk.free / (1024**3)
        print(f" åˆ©ç”¨å¯èƒ½ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡: {disk_gb:.1f}GB")
        
        if disk_gb < 5:
            print(" âš ï¸ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¿…è¦ã§ã™ã€‚")
        
        print()
        
    except ImportError:
        print(" psutilãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
        print(" pip install psutil ã§ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™")
        print()


def check_reazonspeech_model():
    """ReazonSpeechãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ¤ ReazonSpeechãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ:")
    try:
        import nemo.collections.asr as nemo_asr
        from omegaconf import OmegaConf
        
        model_name = "reazon-research/reazonspeech-nemo-v2"
        print(f" ãƒ¢ãƒ‡ãƒ«: {model_name}")
        
        # NeMoãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        print(" NeMoãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        print(" (ã“ã®å‡¦ç†ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...)")
        
        # NeMoãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨èª­ã¿è¾¼ã¿ï¼ˆRNN-Tãƒ¢ãƒ‡ãƒ«ï¼‰
        try:
            model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name)
            model_type = "RNN-T"
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CTCãƒ¢ãƒ‡ãƒ«
            model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name)
            model_type = "CTC"
        
        print(f" âœ… NeMoãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ ({model_type})")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
        if hasattr(model, '_cfg') and hasattr(model._cfg, 'sample_rate'):
            print(f" ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {model._cfg.sample_rate} Hz")
        
        if hasattr(model, 'decoder') and hasattr(model.decoder, 'vocabulary'):
            vocab_size = len(model.decoder.vocabulary) if hasattr(model.decoder.vocabulary, '__len__') else "ä¸æ˜"
            print(f" èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
        elif hasattr(model, 'tokenizer'):
            vocab_size = model.tokenizer.vocab_size if hasattr(model.tokenizer, 'vocab_size') else "ä¸æ˜"
            print(f" èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
        
        print()
        
    except ImportError as e:
        print(f" âŒ NeMoã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(" NeMoãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(" pip install nemo_toolkit[asr] ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print()
    except Exception as e:
        print(f" âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print(" ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        print(" åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
        print()


def check_configuration():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯"""
    print("âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯:")
    try:
        config = ModelConfig()
        print(f" è¨­å®šã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹: {config.device}")
        print(f" è¨­å®šã•ã‚ŒãŸGPU ID: {config.gpu_id}")
        print(f" ãƒ¢ãƒ‡ãƒ«å: {config.model_name}")
        print()
        
        # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’ãƒ†ã‚¹ãƒˆ
        print("éŸ³å£°èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®šç¢ºèª:")
        print(f" è¨­å®šã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹: {config.device}")
        
        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®è¡¨ç¤º
        if config.device.startswith('cuda'):
            import torch
            if torch.cuda.is_available():
                if ':' in config.device:
                    gpu_id = int(config.device.split(':')[1])
                else:
                    gpu_id = torch.cuda.current_device()
                
                gpu_name = torch.cuda.get_device_name(gpu_id)
                gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory / (1024**3)
                
                try:
                    allocated = torch.cuda.memory_allocated(gpu_id) / (1024**3)
                    print(f" GPUè©³ç´°: {gpu_name} (ID: {gpu_id})")
                    print(f" GPU ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB (ç¾åœ¨ä½¿ç”¨ä¸­: {allocated:.1f}GB)")
                except Exception:
                    print(f" GPUè©³ç´°: {gpu_name} (ID: {gpu_id}, ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB)")
            else:
                print(" âš ï¸ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        elif config.device == "mps":
            print(" Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
        else:
            print(" CPUã‚’ä½¿ç”¨")
        
        print()
        
    except Exception as e:
        print(f"è¨­å®šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        print()


def run_comprehensive_test():
    """ç·åˆãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ç·åˆå‹•ä½œãƒ†ã‚¹ãƒˆ:")
    try:
        print(" è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆ...")
        config = ModelConfig()
        print(" âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæˆåŠŸ")
        
        print(" ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—ãƒ†ã‚¹ãƒˆ...")
        device_info = get_device_info()
        print(f" è¨­å®šã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹: {config.device}")
        
        # ä½¿ç”¨ä¸­ã®GPUè©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆè¨­å®šãƒ™ãƒ¼ã‚¹ï¼‰
        if config.device.startswith('cuda'):
            import torch
            if torch.cuda.is_available():
                if ':' in config.device:
                    gpu_id = int(config.device.split(':')[1])
                else:
                    gpu_id = torch.cuda.current_device()
                
                gpu_name = torch.cuda.get_device_name(gpu_id)
                gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory / (1024**3)
                print(f" GPUè©³ç´°: {gpu_name} (ID: {gpu_id}, ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB)")
            else:
                print(" âš ï¸ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        elif config.device == "mps":
            print(" Apple Silicon GPU (MPS) ã‚’ä½¿ç”¨")
        else:
            print(" CPUã‚’ä½¿ç”¨")
        
        print(" âœ… ç·åˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
        print()
        
    except Exception as e:
        print(f" âŒ ç·åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print(" è©³ç´°ãªè¨ºæ–­ã«ã¯å„å€‹åˆ¥ãƒ†ã‚¹ãƒˆã®çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        print()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*50)
    print("ğŸ” MenZ-ReazonSpeech ç’°å¢ƒãƒã‚§ãƒƒã‚¯")
    print("="*50)
    print()
    
    # åŸºæœ¬ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    check_python_environment()
    check_pytorch()
    check_nemo()
    check_audio_libraries()
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
    check_system_resources()
    
    # ReazonSpeechãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯
    check_reazonspeech_model()
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
    check_configuration()
    
    # ç·åˆãƒ†ã‚¹ãƒˆ
    run_comprehensive_test()
    
    print("="*50)
    print("âœ… ç’°å¢ƒãƒã‚§ãƒƒã‚¯å®Œäº†")
    print("="*50)
    print()
    print("æ¨å¥¨äº‹é …:")
    print("- GPUåˆ©ç”¨æ™‚ã¯ååˆ†ãªãƒ¡ãƒ¢ãƒªï¼ˆ8GBä»¥ä¸Šï¼‰ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„")
    print("- åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
    print("- å®‰å®šã—ãŸå‹•ä½œã«ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãŒå¿…è¦ã§ã™")
    print("- éŸ³å£°èªè­˜ã«ã¯é©åˆ‡ãªãƒã‚¤ã‚¯è¨­å®šãŒå¿…è¦ã§ã™")


if __name__ == "__main__":
    main() 